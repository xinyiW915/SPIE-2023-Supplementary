{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained deep learning model\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Define the input image preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "image_path = '/Users/xxxyy/PycharmProjects/UoB/visulisation/video_sampled_frame/7.png'\n",
    "image = Image.open(image_path)\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "# input_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Dictionary to save the features of each layer\n",
    "features = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Define the model hook function to save the features of each layer\n",
    "def save_features(name):\n",
    "    def hook(module, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Register the model hook function\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        hook = module.register_forward_hook(save_features(name))\n",
    "        hooks.append(hook)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 9.7795e-01, -4.3383e+00, -2.5600e-01, -1.9550e+00, -5.3776e-01,\n         -2.3525e+00, -2.1363e+00, -1.6020e+00, -4.9646e-01, -1.4015e+00,\n         -2.6557e+00, -3.6273e+00, -2.9918e+00, -1.6687e+00, -3.3448e+00,\n         -4.3427e+00, -3.3815e+00,  2.9765e-01, -1.2855e-01, -1.0438e+00,\n         -2.8982e+00, -1.7948e+00,  1.0888e+00, -5.6991e-01,  3.5009e+00,\n         -3.0179e+00, -5.6532e+00, -3.0325e+00, -3.0167e+00, -7.2720e+00,\n         -4.4232e+00, -3.9834e+00, -4.6253e+00, -1.0330e+00,  2.1813e+00,\n         -3.3619e+00, -1.9814e+00, -4.0117e+00, -2.9534e+00, -8.7990e-01,\n         -3.4365e+00, -3.3295e+00, -1.9446e+00, -2.2538e+00, -3.9583e+00,\n         -1.0898e+00, -4.3982e+00, -4.9961e+00, -3.0081e+00, -1.7991e+00,\n          6.6251e-01, -1.5115e+00, -3.8117e+00, -3.8402e+00, -3.2338e+00,\n         -5.5393e+00, -1.9041e+00, -5.4470e+00, -2.7533e+00, -3.0394e+00,\n         -4.3945e-02, -2.2833e+00, -2.2868e+00, -1.5722e+00, -3.9776e+00,\n         -3.1006e+00, -4.3494e+00,  1.6845e+00, -3.4055e+00, -2.9461e+00,\n         -3.2251e+00, -2.9088e+00, -2.7590e+00, -3.3169e+00, -3.0534e+00,\n         -4.4613e+00, -4.3693e+00, -4.7556e+00, -3.5283e+00, -3.5032e+00,\n          9.8244e-01,  1.3101e-01,  5.4744e-01, -1.6998e+00, -2.5626e+00,\n         -5.3113e-01, -5.7760e-01,  1.7135e+00, -3.6531e-01,  4.4421e-02,\n         -2.1310e+00, -3.6831e+00, -4.6628e+00, -9.1874e-02, -1.3752e+00,\n         -5.7689e+00, -1.4414e+00, -2.5907e+00, -3.2707e+00, -2.9949e-01,\n         -1.0241e+00, -2.5698e-01, -1.5051e+00, -2.4222e+00, -1.6375e-01,\n          2.0032e-01, -7.9184e-01, -3.2986e+00, -7.7018e+00, -2.8866e+00,\n         -4.9937e+00, -3.5956e+00, -1.1504e+00, -3.7681e+00, -4.0597e+00,\n         -5.1128e+00, -3.1945e+00, -4.6249e+00, -9.8617e-02, -2.0910e+00,\n         -5.9153e+00,  1.8163e+00,  3.5399e-01, -2.0110e+00, -3.2094e-01,\n         -3.5903e+00, -4.5336e+00,  4.6489e-01, -1.4584e+00, -1.2888e+00,\n         -4.9906e+00, -2.6953e+00, -2.2717e+00, -4.4263e+00, -2.5858e+00,\n         -3.3369e+00, -3.3531e+00, -3.5098e+00, -1.1694e+00, -4.9105e+00,\n         -5.5617e+00, -1.8834e+00, -5.6270e+00, -4.2215e+00, -3.1754e+00,\n         -2.4452e+00, -1.2280e+00, -1.3980e+00, -6.1694e-01, -5.2143e+00,\n         -3.1641e+00,  1.9137e+00,  7.6772e-01,  2.6788e+00,  6.7421e-01,\n          1.6008e+00,  1.8678e+00,  1.0905e+00,  6.0028e-01,  2.9715e+00,\n          1.9687e+00,  3.1712e+00,  2.0838e+00,  3.1507e+00,  4.7892e+00,\n          6.8042e-01,  3.1347e+00,  1.5450e+00,  1.3078e+00,  3.2381e+00,\n          5.9819e+00,  2.5385e+00,  4.3200e+00,  2.0737e+00,  4.3839e+00,\n          1.1589e+00,  2.7733e+00,  4.3037e+00,  4.2306e+00,  2.8310e+00,\n          9.9374e-01,  4.2066e+00,  2.4910e+00,  4.1191e+00,  3.1762e+00,\n          1.6671e+00,  1.1663e+00,  7.8769e-01,  2.5746e+00,  3.7906e+00,\n          2.3040e+00,  2.7146e+00,  3.7523e+00, -5.3398e-01,  1.1398e+00,\n          3.0721e+00,  2.5923e+00,  4.0384e+00,  3.3642e+00,  2.8613e+00,\n          2.4355e+00,  1.0474e-01,  4.7151e+00,  4.0183e+00,  2.8699e+00,\n          2.6507e+00,  3.8487e+00,  2.3663e+00,  4.6521e+00,  3.4392e+00,\n          3.7320e+00,  2.0673e+00,  6.1720e+00,  1.2571e+00,  4.6754e+00,\n          3.4850e+00,  5.2741e+00,  4.2227e+00,  2.7413e+00,  4.1050e+00,\n          1.2304e+00,  1.2630e+00,  5.4648e+00,  3.2289e+00,  3.3579e+00,\n          2.0360e+00,  4.4056e+00,  7.9152e-01,  3.6253e+00,  3.8177e+00,\n          2.5704e+00,  1.3664e+00,  1.6481e+00,  4.4934e+00,  3.7976e+00,\n          3.8505e+00,  9.4821e-01,  6.2847e-01,  3.7632e+00,  4.7899e+00,\n          2.6605e+00,  2.1697e+00,  1.9868e+00,  4.2915e-01,  2.7518e+00,\n         -9.6396e-03,  3.3785e+00,  4.3212e+00,  4.8282e+00,  5.5874e+00,\n          4.2597e+00,  5.1201e+00, -6.3131e-01,  1.0636e+00,  2.3677e+00,\n          5.0701e+00,  5.3881e+00,  5.6136e+00,  5.1231e+00,  9.1844e-01,\n          2.0040e+00,  3.4382e+00,  1.9131e-01,  2.1945e+00,  3.9727e+00,\n          4.1072e+00,  3.4302e+00,  6.1337e+00, -1.3806e+00,  1.3505e+00,\n          2.2655e+00, -1.3571e+00,  1.9901e+00,  8.3292e-01, -2.4221e+00,\n         -2.3262e+00, -6.3486e-01, -1.3853e+00, -3.8232e+00,  5.6579e-01,\n         -8.8143e-01, -7.7812e-01, -1.1265e-01, -1.9400e-01, -2.8166e+00,\n         -1.9054e+00,  2.7456e+00,  1.7769e+00,  2.9589e-01,  8.3333e-01,\n         -2.1658e+00, -1.6030e+00, -5.7786e-01,  7.5363e-01, -7.5090e-01,\n          1.7786e+00,  1.0478e+00, -6.6727e-01, -4.3506e+00, -1.7837e+00,\n         -4.7730e+00, -3.5989e+00, -3.4853e+00, -2.5397e+00, -4.5645e+00,\n         -2.0861e+00, -2.1049e+00, -3.2799e+00, -3.4463e+00, -3.8435e+00,\n         -3.1876e+00, -3.3989e+00, -4.0697e+00, -2.3436e+00, -2.8624e+00,\n         -2.7152e+00, -4.1027e+00, -4.0096e+00, -4.4471e+00, -3.4980e+00,\n         -5.8346e+00, -1.3924e+00, -4.0151e+00, -2.9381e+00, -1.5286e+00,\n         -3.1651e+00, -4.2688e+00, -1.0499e+00, -4.0365e+00, -3.3468e+00,\n          7.4981e-01,  1.5248e-02,  7.2410e-01, -4.0136e+00, -4.6259e-02,\n         -3.3608e+00, -1.3862e+00, -2.1553e+00, -3.0753e+00, -2.9338e+00,\n         -4.8998e-01, -1.2943e+00, -8.2788e-01, -1.6961e+00, -5.6297e+00,\n         -9.3873e-01, -1.7768e+00,  3.9477e-01,  4.1295e-01, -6.5732e-01,\n         -6.2201e-01, -3.1738e+00, -2.4805e+00, -4.0859e+00,  1.3219e+00,\n          2.0885e+00,  1.1659e+00, -2.5479e+00, -7.9547e-01, -2.1435e+00,\n         -3.6145e+00,  4.1325e-01, -1.9899e+00,  7.9623e-01, -1.9868e+00,\n         -3.0684e+00, -2.6085e+00, -1.8318e+00, -2.8452e+00, -1.9901e+00,\n         -3.6528e+00, -2.5374e+00, -2.3075e+00, -5.1876e-01, -1.9776e+00,\n         -1.3984e+00, -4.0682e+00, -3.5126e+00, -1.1852e+00, -1.2118e+00,\n         -3.0393e+00, -1.5084e+00, -2.4636e+00, -1.1479e+00, -7.7514e-01,\n         -7.1272e-01, -1.7306e+00, -3.3768e+00, -3.5559e-01,  1.6386e+00,\n         -2.3081e+00,  4.6497e+00, -4.1237e+00, -5.2829e+00,  3.3790e+00,\n          1.0439e+00, -3.1505e+00, -4.1369e+00, -1.5572e-01,  2.4090e+00,\n          1.0547e+00,  4.1229e+00,  1.9300e+00, -3.7818e+00, -3.3158e+00,\n         -1.2476e-01, -4.9295e-01,  1.6204e+00, -1.7707e+00,  2.0055e+00,\n          3.5226e+00,  5.7448e+00,  1.9111e+00,  2.8543e+00,  5.9547e+00,\n          2.7416e-01,  2.8355e+00,  1.4839e+00, -3.1965e+00, -5.7220e-01,\n          3.4569e+00,  3.6180e+00,  1.0741e-01,  9.5152e-01,  6.1223e-01,\n          2.8664e+00, -9.2528e-01,  7.1226e-02,  5.3236e+00,  2.5275e+00,\n          6.8785e-01,  2.5392e+00, -3.7602e-01, -6.9050e-01,  1.0875e+00,\n          1.9830e+00,  1.6375e-01,  2.4540e+00, -1.9269e+00,  1.9927e+00,\n         -1.1723e+00, -2.4165e+00,  1.5058e+00,  2.0029e+00,  4.1294e+00,\n          7.9774e-02, -3.8919e+00,  3.3764e+00,  3.5958e+00, -8.4763e-01,\n          6.7524e+00,  1.6021e+00,  2.5222e+00, -1.9304e+00, -1.8367e+00,\n         -2.0204e+00,  2.9505e+00,  3.6665e+00,  1.4293e+00,  2.6068e+00,\n          1.7675e+00,  3.4306e+00,  4.9938e+00,  4.5018e+00, -4.5594e-01,\n          1.3185e+00, -9.2725e-01, -1.6649e+00,  4.3954e-01,  1.0908e+00,\n         -1.3779e+00,  5.3653e-01,  3.4746e+00, -2.5867e+00,  4.9823e+00,\n          8.2516e-01, -2.7203e+00, -2.7666e+00, -1.5848e+00, -3.1164e-01,\n          3.4083e+00, -3.0807e+00, -1.0245e+00,  1.1351e+00, -9.8933e-01,\n         -2.8711e+00, -1.4542e-01,  2.1966e+00,  3.9250e-01,  3.5691e+00,\n          2.8990e+00,  7.7800e+00, -1.4642e+00, -2.9657e+00,  1.2678e+00,\n         -2.3645e+00,  3.7154e+00,  2.7238e+00, -1.7093e+00,  2.5867e+00,\n          9.8540e-01,  7.3278e+00, -7.4314e-01, -1.0763e+00, -1.0320e+00,\n         -3.2147e-01, -1.2215e+00, -2.2097e+00, -2.8717e+00, -3.6580e+00,\n         -2.4159e+00, -2.4750e+00, -1.5638e+00,  1.1264e+00,  4.2020e+00,\n          4.0183e+00,  6.7095e-01, -5.4165e-01,  4.2381e+00, -3.3036e-01,\n         -1.5364e+00, -1.2685e+00,  1.9568e+00,  5.5411e+00,  2.1137e+00,\n          1.6918e+00, -2.5241e+00, -3.1995e+00,  2.3317e-01,  1.4351e+00,\n          2.8778e-01, -4.1597e-02, -2.9218e+00, -2.3262e+00, -8.3037e-02,\n         -2.8106e+00, -3.7233e+00,  7.7591e+00,  2.4849e+00,  9.5665e-01,\n         -9.0107e-01,  1.4075e+00,  1.5619e+00,  8.5326e-01,  1.8823e+00,\n          1.1340e+00,  2.6285e+00, -9.6799e-01, -2.6529e+00, -2.5403e-01,\n         -2.9593e+00, -2.3573e+00,  1.2654e+00, -1.2868e+00,  4.7564e-02,\n          1.5386e+00, -3.6097e+00,  4.9310e+00,  1.3996e+00,  7.8429e-01,\n          3.3051e+00, -1.1411e+00,  4.0185e-01, -5.8153e+00, -2.9459e-01,\n         -2.6253e+00,  1.2084e+00,  9.0058e-01,  6.1737e+00, -4.7405e-01,\n          4.5014e+00,  1.5820e+00, -1.8466e+00, -3.3843e-01,  2.7659e+00,\n          1.7544e+00, -1.3993e-01,  1.2923e+00,  4.4863e+00,  6.5865e-01,\n          4.2419e-01, -2.2763e+00, -4.4103e+00,  8.4475e-01, -1.5408e+00,\n          7.6628e-01, -1.2489e+00,  3.7112e+00,  4.3418e-01,  2.9434e+00,\n         -1.6162e+00,  1.8533e+00, -5.6140e+00,  1.5312e+00, -7.1742e-02,\n         -1.2558e+00,  4.3542e+00,  2.3253e+00, -2.9370e+00,  9.1636e-01,\n          1.3990e+00,  6.4581e+00,  2.5799e+00, -2.3291e-01, -3.4467e+00,\n         -3.1650e-01,  1.4936e+00,  1.1025e-04,  4.7370e+00,  1.0473e+00,\n          8.1610e+00, -1.1866e+00,  5.1366e+00, -2.6051e+00,  7.8218e+00,\n          5.0959e+00,  8.8199e-01,  4.0096e+00,  4.0486e-01, -1.2129e+00,\n          4.8907e-01,  5.7280e+00,  1.3022e+00, -2.5020e+00, -1.1763e+00,\n         -1.7137e+00, -1.9576e+00, -7.8443e-01, -2.7247e+00, -3.7829e+00,\n          8.9466e-01, -2.1421e+00, -1.6646e-01, -1.2796e+00, -2.6817e-01,\n         -2.5154e+00,  3.4399e+00,  3.7920e+00,  2.4561e+00,  8.3948e-01,\n          3.6384e-01,  2.1157e+00,  2.3163e+00,  3.4332e+00, -1.9530e+00,\n          3.0629e+00,  1.8855e+00, -2.2355e+00, -3.5623e+00,  2.7247e+00,\n          2.5953e+00, -3.0443e-01,  3.8329e+00,  1.5628e+00,  3.6963e-01,\n          4.6179e+00,  2.6874e+00, -7.9432e-01,  5.2470e+00, -1.4023e+00,\n          1.3842e+00, -1.2523e+00, -4.8098e+00,  1.8727e+00, -2.1917e+00,\n          2.4021e+00, -1.9556e+00,  1.5208e+00,  4.2597e-01, -2.9602e-01,\n          2.9421e+00,  4.1533e+00,  4.5988e+00, -2.5320e+00, -1.5580e+00,\n         -3.4905e-01,  2.8231e+00, -7.8771e-01,  3.4092e+00, -2.2583e+00,\n         -7.7115e-02,  1.3961e+00,  1.8801e-01,  1.1099e+00, -8.3654e-01,\n         -3.5237e+00, -4.1819e+00, -3.7430e+00, -3.9514e+00,  6.1293e+00,\n         -1.1464e+00,  4.7229e-01, -2.0710e+00,  4.4882e+00, -9.4888e-01,\n         -9.2958e-01,  2.4753e+00,  5.7918e+00,  8.2383e-02,  9.0617e-01,\n          2.1934e+00,  2.9345e+00,  2.8734e+00,  3.8009e+00,  3.6330e+00,\n         -4.0258e-01, -4.7337e-01,  1.6252e+00,  3.4528e-01, -2.0603e+00,\n         -3.0399e+00, -2.0097e+00, -3.7882e+00, -1.1430e+00, -8.6963e-01,\n          1.6295e+00,  4.5343e+00, -1.1071e+00, -3.0017e-01, -1.5560e+00,\n         -1.6012e+00,  1.0521e-01,  2.4551e+00,  3.1845e+00, -8.6102e-01,\n         -1.7286e+00, -4.2656e+00,  3.2302e-02,  3.4693e+00, -1.9033e+00,\n          1.6621e+00,  4.2093e+00, -9.1294e-01,  2.9683e+00, -8.4471e-01,\n          6.8437e+00, -3.4754e-01, -1.2289e+00, -1.8640e+00, -2.5300e-01,\n          3.8378e+00,  1.7528e+00, -2.0341e+00,  1.5617e+00, -2.1499e+00,\n         -2.4689e+00,  5.4148e+00,  3.0090e+00, -1.7370e-01,  4.5577e-01,\n          1.3115e+00, -6.5856e-01,  3.9605e+00, -1.6483e+00, -1.1822e+00,\n          4.3229e+00,  1.1565e+00,  5.5992e-01,  4.2949e+00, -4.0910e-01,\n          4.7642e-01, -1.6538e-01, -1.0681e+00,  2.0156e+00,  4.5341e+00,\n          1.6216e+00, -2.2059e+00, -2.0501e+00,  3.0629e+00,  7.3850e-01,\n          2.2845e+00, -2.2008e+00, -1.0610e-01, -2.6958e+00, -1.2611e+00,\n          1.8356e+00,  2.2129e+00,  2.7264e+00,  7.3850e-02,  1.6664e+00,\n         -1.0981e+00,  1.5268e+00, -4.7086e-01, -3.3335e+00, -6.6928e-01,\n         -3.9250e-01, -1.3665e+00,  5.0399e+00, -2.2859e+00,  2.0112e+00,\n         -1.1316e+00,  4.6309e+00,  1.2005e+01,  2.4544e+00, -1.9191e+00,\n          9.7379e+00,  8.5310e+00,  5.5562e+00, -6.4901e-01,  3.4842e-01,\n         -1.5225e+00, -1.1558e+00,  6.8258e+00,  4.2510e+00, -1.9825e+00,\n          8.4351e+00,  1.5776e+00,  3.1552e+00,  2.2374e+00, -3.4751e+00,\n         -5.3153e+00, -8.6977e-01,  1.3992e-01,  6.2952e+00, -1.2078e+00,\n         -1.1823e+00, -2.0343e+00, -1.8536e+00,  2.3528e+00,  9.3258e-01,\n          6.6727e-01, -3.1326e-01,  4.2740e+00,  3.4076e+00,  5.7731e+00,\n          3.8427e+00,  5.2419e-01, -9.4012e-01,  9.0921e-01, -5.6816e-01,\n          3.2240e+00, -3.8720e-01,  3.2119e+00, -2.5058e+00,  2.0979e+00,\n         -2.2056e-01,  2.9709e+00,  3.7095e+00, -1.4777e+00,  4.1490e+00,\n          4.6517e+00,  9.8127e+00,  3.3479e+00,  5.1943e+00, -1.5648e+00,\n         -9.0448e-01, -2.9249e+00, -6.7024e-01, -1.1323e+00, -1.9084e+00,\n          3.6006e+00,  9.4403e-01,  1.2764e+00,  6.0012e-01, -8.9665e-01,\n         -1.8745e+00, -8.3252e-01,  5.4364e-01, -2.3754e+00,  6.0429e-01,\n         -2.4449e+00,  5.4979e-01,  2.4019e+00,  2.1458e+00,  1.7952e-02,\n         -1.1476e+00,  1.0899e+00, -1.1545e+00,  6.0371e-01,  6.8719e+00,\n          6.5604e+00, -2.7475e+00,  5.1700e+00,  1.3378e+00, -1.8697e+00,\n          7.0893e-01,  1.7154e+00, -5.8359e-01, -6.1101e+00,  6.3002e+00,\n          6.9089e+00, -1.9869e-01,  3.7204e+00, -1.8062e+00, -1.4952e+00,\n          4.5690e+00, -2.1045e-01,  3.9280e+00,  1.5421e-01,  1.2471e+00,\n          1.4110e+00, -4.9385e-01, -6.4719e-01, -4.0606e+00, -9.9288e-02,\n         -2.2509e+00,  6.0935e-02,  1.3997e+00,  3.8831e+00,  2.0231e+00,\n          1.3172e+00, -2.8769e+00,  1.4956e+00,  2.0965e+00,  5.0605e-01,\n         -4.9328e-01,  3.5767e-01, -2.0310e+00, -2.2148e+00, -4.1825e-01,\n          1.9850e+00,  3.1923e+00,  1.9532e+00, -1.5289e+00, -2.7039e+00,\n          4.5103e+00, -5.7913e-01, -2.1682e+00,  1.1903e+00,  3.2858e+00,\n          2.0754e+00, -1.3505e-01, -3.8732e+00, -1.9609e+00, -2.7089e+00,\n         -6.1753e+00, -5.6662e+00, -8.8260e-01, -1.5809e+00,  1.5915e+00,\n         -4.8117e+00, -2.2476e+00, -8.3368e-01, -2.8008e+00, -1.4104e+00,\n         -2.5455e+00, -1.6471e+00, -2.8592e+00, -1.0193e+00, -3.0335e+00,\n         -4.4204e+00, -4.6711e+00, -4.7546e+00, -4.4283e+00, -5.5373e+00,\n         -5.4712e+00, -6.6350e+00, -2.2832e+00, -1.0811e+00, -2.5898e+00,\n         -1.9826e+00, -1.4911e+00, -4.1903e+00, -1.1250e+00, -3.2484e+00,\n         -2.8249e+00, -1.7204e+00, -2.7680e+00,  2.0600e+00, -4.1325e+00,\n         -2.9781e+00, -6.4091e-02, -1.7392e+00, -2.6326e+00, -4.6489e+00,\n         -1.1913e+00, -1.3362e+00, -3.6707e+00, -1.9696e+00, -3.2090e+00,\n          4.9985e+00,  4.2589e-01,  3.5744e+00, -3.7180e+00,  1.9507e+00,\n          2.2022e+00,  1.6003e-01,  4.3367e-01, -5.7338e-01,  1.6070e+00,\n          9.0030e-01,  4.3899e+00,  2.6999e+00, -6.5925e-01,  1.9905e+00,\n         -3.3516e+00, -3.8181e+00, -1.0855e+00, -4.5259e+00, -6.0100e-01,\n         -3.8845e+00, -4.3026e+00, -1.9720e+00, -4.0347e+00, -1.9108e+00,\n         -2.2301e+00, -8.6530e-01, -3.1060e+00, -5.8322e-01,  2.1426e+00]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the pre-trained model for forward propagation\n",
    "output = model(input_batch)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Create a folder to save the feature maps\n",
    "save_folder = '/Users/xxxyy/PycharmProjects/UoB/visulisation/vgg/deep_feautures_visulisation_2D_max/'\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['features.0', 'features.2', 'features.5', 'features.7', 'features.10', 'features.12', 'features.14', 'features.17', 'features.19', 'features.21', 'features.24', 'features.26', 'features.28'])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Iterate over each layer and save the slice images\n",
    "for name, output in features.items():\n",
    "    # Get the size of the feature map\n",
    "    num_channels, height, width = output.shape[1:]\n",
    "\n",
    "    # Find the channel with the maximum activation\n",
    "    max_channel = torch.argmax(output.view(num_channels, -1).mean(dim=1))\n",
    "\n",
    "    # Get the slice image of the maximum activation channel\n",
    "    slice_image = output[0, max_channel, :, :].cpu().numpy()\n",
    "\n",
    "    # Generate the save path\n",
    "    save_path = os.path.join(save_folder, f'{name}_max_activation.png')\n",
    "\n",
    "    # Add title to the plot\n",
    "    plt.imshow(slice_image, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{name}__max activation', fontsize=26)  # Add the title here\n",
    "\n",
    "    # Save the slice image\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# plot all the max activation feature maps\n",
    "import math\n",
    "\n",
    "# Calculate the number of rows and columns in the final grid\n",
    "num_images = len(features)\n",
    "num_columns = 7\n",
    "num_rows = math.ceil(num_images / num_columns)\n",
    "\n",
    "# Create a new figure with the appropriate size\n",
    "fig, axs = plt.subplots(num_rows, num_columns, figsize=(16, 16))\n",
    "\n",
    "# Iterate over each layer and save the slice images\n",
    "for i, (name, output) in enumerate(features.items()):\n",
    "    # Get the size of the feature map\n",
    "    num_channels, height, width = output.shape[1:]\n",
    "\n",
    "    # Find the channel with the maximum activation\n",
    "    max_channel = torch.argmax(output.view(num_channels, -1).mean(dim=1))\n",
    "\n",
    "    # Get the slice image of the maximum activation channel\n",
    "    slice_image = output[0, max_channel, :, :].cpu().numpy()\n",
    "\n",
    "    # Calculate the row and column index for the current image\n",
    "    row_index = i // num_columns\n",
    "    col_index = i % num_columns\n",
    "\n",
    "    # Plot the slice image on the corresponding subplot\n",
    "    axs[row_index, col_index].imshow(slice_image, cmap='viridis')\n",
    "    axs[row_index, col_index].axis('off')\n",
    "\n",
    "    # Add text label with the coordinates\n",
    "    axs[row_index, col_index].text(0.5, -0.1, f'({name})',\n",
    "                                   transform=axs[row_index, col_index].transAxes,\n",
    "                                   fontsize=15, ha='center')\n",
    "\n",
    "# Remove any empty subplots\n",
    "if num_images < num_rows * num_columns:\n",
    "    for i in range(num_images, num_rows * num_columns):\n",
    "        row_index = i // num_columns\n",
    "        col_index = i % num_columns\n",
    "        axs[row_index, col_index].axis('off')\n",
    "\n",
    "# Add title to the plot\n",
    "fig.suptitle('Max Activation Feature Maps for different layers of VGG16', fontsize=26)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = os.path.join(save_folder, 'vgg16_max_activation_all_images.eps')\n",
    "plt.savefig(save_path)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Remove the model hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}